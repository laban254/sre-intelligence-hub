{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Distributed Data Processing with PySpark\n",
                "\n",
                "## Objectives\n",
                "- Bridge the gap between local tools (Pandas) and infrastructure-level big data processing (`PySpark`).\n",
                "- Understand the map-reduce paradigm for processing logs or metrics that won't fit into your local machine's RAM.\n",
                "- Set up a local Spark Session to aggregate and query dataset sizes that typically require a cluster.\n",
                "\n",
                "## Expected Outcome\n",
                "- A functional local PySpark pipeline capable of grouping and extracting metrics from millions of log rows.\n",
                "\n",
                "## Challenge\n",
                "- Rewrite a standard Pandas DataFrame `groupby()` utilizing PySpark RDDs or Spark DataFrames."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install pyspark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Initializing Spark\n",
                "Unlike Pandas, Spark requires an active \"Session\" or \"Context\" that connects your Python code to the JVM (Java Virtual Machine) backend that does the heavy lifting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, month, count, avg\n",
                "\n",
                "# Set up a local spark session using all available CPU cores (*)\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"InfraLogAnalysis\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .getOrCreate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Simulating a Gigantic Log File\n",
                "We'll generate a dummy dataset of server events that resembles a real-world ELK stack export."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Create 500,000 synthetic log records\n",
                "num_records = 500000\n",
                "sample_services = ['auth-service', 'billing-api', 'frontend-ui', 'database-pg']\n",
                "sample_levels = ['INFO', 'WARN', 'ERROR', 'DEBUG']\n",
                "\n",
                "data = {\n",
                "    \"timestamp\": pd.date_range(start=\"2025-01-01\", periods=num_records, freq=\"15S\"),\n",
                "    \"service\": np.random.choice(sample_services, num_records, p=[0.4, 0.2, 0.3, 0.1]),\n",
                "    \"log_level\": np.random.choice(sample_levels, num_records, p=[0.7, 0.1, 0.05, 0.15]),\n",
                "    \"response_time_ms\": np.random.gamma(shape=2.0, scale=50.0, size=num_records)\n",
                "}\n",
                "\n",
                "# Convert Pandas DataFrame to PySpark DataFrame\n",
                "pdf = pd.DataFrame(data)\n",
                "df = spark.createDataFrame(pdf)\n",
                "\n",
                "print(\"Spark DataFrame Schema:\")\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Distributed Queries (Lazy Evaluation)\n",
                "In Spark, defining a query doesn't execute it immediately. Execution happens only when an action (like `.show()` or `.collect()`) is called."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregating average response times and error counts by service\n",
                "# If this was a 100GB dataset, this query would farm out to your cluster implicitly\n",
                "service_metrics = df.filter(col(\"log_level\").isin([\"ERROR\", \"WARN\"])) \\\n",
                "    .groupBy(\"service\", \"log_level\") \\\n",
                "    .agg(\n",
                "        count(\"*\").alias(\"total_occurrences\"),\n",
                "        avg(\"response_time_ms\").alias(\"avg_latency_ms\")\n",
                "    ) \\\n",
                "    .orderBy(\"total_occurrences\", ascending=False)\n",
                "\n",
                "print(\"Top Services with Warnings and Errors:\")\n",
                "service_metrics.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop the Spark context to free up memory\n",
                "spark.stop()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}