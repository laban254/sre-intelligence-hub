{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Manipulation with Pandas\n",
                "\n",
                "## Context\n",
                "NumPy is great for pure mathematical operations on matrices, but as an SRE, your data is messy. You receive CSV exports of billing data, JSON dumps of server inventory, and unstructured application logs. You need a tool that handles missing data, column names, and time-series alignments natively.\n",
                "\n",
                "## Objectives\n",
                "- Understand the core Pandas data structures: `Series` (1D) and `DataFrame` (2D).\n",
                "- Learn how to clean messy infrastructure data (handling NaN values and dropping duplicates).\n",
                "- Perform SQL-like aggregations (groupby) and time-series resampling on server metrics.\n",
                "\n",
                "## Expected Outcome\n",
                "- The ability to take a raw, messy csv of server logs, clean it, group it by microservice, and output actionable aggregated metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Pandas Data Structures\n",
                "Think of a `DataFrame` exactly like a SQL table or an Excel spreadsheet. It has rows, named columns, and an index. A single column extracted from a DataFrame is called a `Series`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating a DataFrame from a dictionary (Simulating an active server inventory)\n",
                "inventory_data = {\n",
                "    'Hostname': ['web-01', 'web-02', 'db-01', 'worker-01'],\n",
                "    'Role': ['Frontend', 'Frontend', 'Database', 'Background'],\n",
                "    'CPU_Cores': [4, 4, 16, 8],\n",
                "    'Status': ['Active', 'Active', 'Maintenance', 'Active']\n",
                "}\n",
                "df_inventory = pd.DataFrame(inventory_data)\n",
                "print(\"Server Inventory DataFrame:\\n\")\n",
                "print(df_inventory)\n",
                "\n",
                "# Extracting a single Series (just the Hostnames)\n",
                "hostnames = df_inventory['Hostname']\n",
                "print(\"\\nHostname Series (type: \", type(hostnames), \"):\")\n",
                "print(hostnames)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Data Cleaning (Handling Messy Logs)\n",
                "In the real world, instrumentation fails. Metrics get dropped. You will frequently encounter `NaN` (Not a Number) values. Pandas gives you tools to handle these safely."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulating incomplete metrics reporting from an agent cluster\n",
                "messy_metrics = pd.DataFrame({\n",
                "    'Server': ['app-1', 'app-2', 'app-3', 'app-1'],  # Notice the duplicate app-1\n",
                "    'Memory_Usage_%': [85.5, np.nan, 42.1, 85.5],    # Missing value for app-2\n",
                "    'Disk_IO_MBps': [120, 15, np.nan, 120]\n",
                "})\n",
                "print(\"Raw Messy Data:\\n\", messy_metrics)\n",
                "\n",
                "# 1. Drop exact duplicate rows (retries from the metrics agent)\n",
                "df_clean = messy_metrics.drop_duplicates()\n",
                "print(\"\\nAfter dropping duplicates:\\n\", df_clean)\n",
                "\n",
                "# 2. Handle missing data\n",
                "# Option A: Forward-fill or interpolate (dangerous for critical alerting, good for smooth charts)\n",
                "# Option B: Fill with a safe default (e.g., 0)\n",
                "# Option C: Drop the row entirely\n",
                "df_filled = df_clean.fillna(\n",
                "    {'Memory_Usage_%': df_clean['Memory_Usage_%'].mean(),  # Impute with average\n",
                "     'Disk_IO_MBps': 0}                                    # Assume 0 if not reported\n",
                ")\n",
                "print(\"\\nAfter handling missing values:\\n\", df_filled)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Data Transformation\n",
                "Often you need to derive new metrics from existing ones, or rename columns to match a standard schema (e.g., standardizing on Prometheus labels)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Renaming columns to standard labels\n",
                "df_renamed = df_inventory.rename(columns={'Hostname': 'instance', 'Role': 'service'})\n",
                "\n",
                "# Creating a new derived column (e.g., mapping Cores to an estimated monthly cost)\n",
                "core_cost_per_month = 15.00\n",
                "df_renamed['Estimated_Cost_$'] = df_renamed['CPU_Cores'] * core_cost_per_month\n",
                "\n",
                "print(df_renamed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Grouping and Aggregation\n",
                "`groupby` operations are the backbone of exploratory data analysis. They function exactly like SQL `GROUP BY` statements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulating 10,000 requests processed by 3 microservices\n",
                "np.random.seed(42)\n",
                "requests_df = pd.DataFrame({\n",
                "    'Service': np.random.choice(['Auth', 'Payments', 'Search'], 10000),\n",
                "    'Latency_ms': np.random.gamma(2, 50, 10000),\n",
                "    'Status_Code': np.random.choice([200, 400, 500], 10000, p=[0.9, 0.08, 0.02])\n",
                "})\n",
                "\n",
                "# Which service is the slowest on average?\n",
                "avg_latency = requests_df.groupby('Service')['Latency_ms'].mean().sort_values(ascending=False)\n",
                "print(\"Average Latency by Service:\\n\", avg_latency)\n",
                "\n",
                "# Complex aggregation: Get count, mean latency, and 95th percentile by service\n",
                "def p95(x):\n",
                "    return x.quantile(0.95)\n",
                "\n",
                "service_metrics = requests_df.groupby('Service').agg({\n",
                "    'Latency_ms': ['count', 'mean', p95],\n",
                "    'Status_Code': lambda x: (x == 500).sum()  # Count 500 errors\n",
                "})\n",
                "print(\"\\nDetailed Service Metrics:\\n\", service_metrics)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Time-Series Data Handling\n",
                "Pandas was originally built for financial modeling, so its time-series capabilities are unmatched. This is vital for analyzing Prometheus or Datadog exports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate a time series of CPU Temperature readings taken every 10 seconds for 1 hour\n",
                "time_index = pd.date_range(start='2025-02-25 00:00:00', periods=360, freq='10S')\n",
                "cpu_temps = np.random.normal(loc=45, scale=5, size=360) + np.linspace(0, 15, 360) # Slowly overheating\n",
                "\n",
                "temp_df = pd.DataFrame({'Temp_C': cpu_temps}, index=time_index)\n",
                "\n",
                "print(\"Raw 10-second resolution data (first 3 rows):\\n\", temp_df.head(3))\n",
                "\n",
                "# Resampling: Downsample from 10-second data to 5-minute averages\n",
                "# This is exactly what Prometheus does when you zoom out on a Grafana dashboard!\n",
                "downsampled = temp_df.resample('5T').mean()\n",
                "print(\"\\nDownsampled 5-minute averages:\\n\", downsampled)\n",
                "\n",
                "# Simple built-in plotting\n",
                "plt.figure(figsize=(10, 4))\n",
                "temp_df['Temp_C'].plot(alpha=0.5, label='Raw (10s)')\n",
                "downsampled['Temp_C'].plot(color='red', linewidth=2, label='Avg (5min)')\n",
                "plt.title(\"Server CPU Temperature Analysis\")\n",
                "plt.ylabel(\"Degrees (C)\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Merging and Joining (Correlating Data)\n",
                "SREs rarely get all answers from one system. You might need to join billing export data (CSV) with current usage metrics (API JSON) to find the cost per request."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The Inventory Data (from earlier)\n",
                "print(\"Inventory:\\n\", df_renamed[['instance', 'Estimated_Cost_$']])\n",
                "\n",
                "# New Metrics Data (e.g., pulled from Thanos)\n",
                "usage_data = pd.DataFrame({\n",
                "    'instance': ['web-01', 'web-02', 'db-01'],\n",
                "    'Total_Requests': [1500000, 1450000, 8000000]\n",
                "})\n",
                "\n",
                "# Merge them (Inner Join by default)\n",
                "merged_df = pd.merge(df_renamed, usage_data, on='instance', how='left')\n",
                "\n",
                "# Calculate cost per million requests\n",
                "merged_df['Cost_Per_1M_Req'] = (merged_df['Estimated_Cost_$'] / merged_df['Total_Requests']) * 1000000\n",
                "\n",
                "print(\"\\nMerged Business Correlation:\\n\", merged_df[['instance', 'Role', 'Cost_Per_1M_Req']])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}