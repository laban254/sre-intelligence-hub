{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Dimensional Arrays with NumPy\n",
                "\n",
                "## Context\n",
                "As an SRE, you often deal with massive streams of numerical data: CPU utilization percentages, network throughput in bytes, or memory consumption across hundreds of containers. Python lists are far too slow and memory-inefficient to handle this at scale.\n",
                "\n",
                "## Objectives\n",
                "- Understand NumPy's core object: the `ndarray` (N-dimensional array).\n",
                "- Learn how to perform vectorized operations on arrays without slow `for` loops.\n",
                "- Apply basic statistical functions to uncover anomalies in synthetic infrastructure metrics.\n",
                "\n",
                "## Expected Outcome\n",
                "- The ability to quickly analyze thousands of datapoints, calculate baselines, and detect spikes using pure mathematics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Creating Arrays (Simulating Metrics)\n",
                "An `ndarray` allows you to store and manipulate large datasets efficiently. Let's create some arrays simulating CPU load percentages across different servers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1D Array: CPU load recorded every minute for 1 server\n",
                "cpu_server_1 = np.array([45, 48, 52, 50, 47])\n",
                "print(\"1D Array (Server 1):\", cpu_server_1)\n",
                "\n",
                "# 2D Array (Matrix): CPU load for 3 separate servers over 4 minutes\n",
                "cpu_cluster = np.array([\n",
                "    [45, 48, 52, 50], # Server A\n",
                "    [20, 22, 21, 23], # Server B\n",
                "    [85, 88, 90, 89]  # Server C (Spiking!)\n",
                "])\n",
                "print(\"\\n2D Array (Cluster Metrics):\\n\", cpu_cluster)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Inspecting Array Properties\n",
                "When you load a dataset from a log file, the first step is understanding its dimensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Shape of cpu_cluster:\", cpu_cluster.shape)  # Output: (3 servers, 4 minutes)\n",
                "print(\"Number of dimensions:\", cpu_cluster.ndim)\n",
                "print(\"Total data points:\", cpu_cluster.size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Generating Data\n",
                "Often you need to generate placeholder metrics or baseline limits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an array representing a 0% load baseline for 5 servers\n",
                "zeros_array = np.zeros(5)\n",
                "print(\"Zero Baseline:\\n\", zeros_array)\n",
                "\n",
                "# Create an array representing a max capacity limit (100%)\n",
                "max_capacity = np.full((3, 4), 100)\n",
                "print(\"\\nMax Capacity Matrix:\\n\", max_capacity)\n",
                "\n",
                "# Generate a sequence of timestamps (e.g., 0 to 60 seconds, step 10)\n",
                "timestamps = np.arange(0, 60, 10)\n",
                "print(\"\\nTimestamps:\", timestamps)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Indexing and Slicing (Isolating Servers)\n",
                "If Server C is throwing alerts, we need to extract only its data from the matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Accessing specific elements\n",
                "print(\"Load for Server B at Minute 2:\", cpu_cluster[1, 1]) \n",
                "\n",
                "# Slicing entire rows/columns\n",
                "print(\"All metrics for Server C:\", cpu_cluster[2, :])\n",
                "print(\"Metrics across all servers at Minute 3:\", cpu_cluster[:, 2])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Element-wise Operations & Broadcasting\n",
                "NumPy's greatest strength is performing mathematical operations on entire arrays at once, without `for` loops. This is called *vectorization*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ram_used_gb = np.array([16, 32, 8])\n",
                "ram_total_gb = np.array([64, 64, 16])\n",
                "\n",
                "# Element-wise calculation to find utilization percentage\n",
                "utilization = (ram_used_gb / ram_total_gb) * 100\n",
                "print(\"RAM Utilization %:\", utilization)\n",
                "\n",
                "# Broadcasting: Subtracting a scalar from an array\n",
                "# Imagine a new software update reduces RAM usage by 2GB across all servers\n",
                "new_ram_used = ram_used_gb - 2\n",
                "print(\"\\nNew RAM Usage:\", new_ram_used)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Statistical Operations (Finding Anomalies)\n",
                "Using built-in stats functions allows you to immediately identify outliers in your cluster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Matrix:\\n\", cpu_cluster)\n",
                "\n",
                "# Global stats\n",
                "print(\"\\nGlobal Max CPU Load:\", np.max(cpu_cluster))\n",
                "print(\"Global Mean CPU Load:\", np.mean(cpu_cluster))\n",
                "\n",
                "# Axis-specific stats (0 = columns, 1 = rows)\n",
                "# Calculate the mean load for each individual server\n",
                "server_means = np.mean(cpu_cluster, axis=1)\n",
                "print(\"\\nMean Load per Server:\", server_means)\n",
                "\n",
                "# Calculate the 99th percentile across all data (crucial for SLAs)\n",
                "p99 = np.percentile(cpu_cluster, 99)\n",
                "print(\"99th Percentile CPU Load:\", p99)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Boolean Masks (Filtering Alerts)\n",
                "We can use arrays of booleans to filter raw data and retrieve only the values that breach our thresholds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all instances where CPU load exceeded 80%\n",
                "alert_mask = cpu_cluster > 80\n",
                "print(\"Alert Mask:\\n\", alert_mask)\n",
                "\n",
                "# Use the mask to extract those specific dangerous values\n",
                "dangerous_values = cpu_cluster[alert_mask]\n",
                "print(\"\\nValues exceeding 80%:\", dangerous_values)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}