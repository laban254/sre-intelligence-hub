{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Visualizing Infrastructure Metrics with Matplotlib\n",
                "\n",
                "## Context\n",
                "As an SRE, you constantly look at Grafana dashboards, Datadog charts, and AWS CloudWatch metrics. But what happens when you need to generate a custom report for a post-mortem, or visualize a highly specific metric that isn't supported by out-of-the-box tools? You write it yourself using Matplotlib.\n",
                "\n",
                "## Objectives\n",
                "- Learn how to generate the 3 fundamental chart types used in SRE: Line plots (Time-series), Bar charts (Distribution), and Histograms (Latency buckets).\n",
                "- Understand how to add context (threshold lines, annotations) to make charts readable during high-stress incidents.\n",
                "\n",
                "## Expected Outcome\n",
                "- You will be able to take raw metric data and turn it into clear, professional visualizations ready for a Root Cause Analysis (RCA) document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. The Line Plot (Time-Series Metrics)\n",
                "Line plots are the absolute core of observability. You use them for CPU, Memory, Network traffic, and Error rates over time. Here we will simulate a memory leak over a 24-hour period."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate 24 hours of memory utilization (%) reading every hour\n",
                "hours = np.arange(0, 24)\n",
                "base_memory = 40\n",
                "leak = np.linspace(0, 50, 24)        # Memory slowly creeps up\n",
                "noise = np.random.normal(0, 3, 24)   # Add realistic fluctuation\n",
                "memory_utilization = base_memory + leak + noise\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "# Create the main plot\n",
                "plt.plot(hours, memory_utilization, marker='o', linestyle='-', color='b', label='app-server-1 Memory %')\n",
                "\n",
                "# Adding context (CRITICAL for RCA documents)\n",
                "plt.axhline(y=80, color='r', linestyle='--', label='80% Alert Threshold')\n",
                "plt.axvline(x=18, color='orange', linestyle=':', label='OOM Kill Event')\n",
                "\n",
                "plt.title('Memory Utilization Over 24h (Suspected Memory Leak)')\n",
                "plt.xlabel('Hour of Day')\n",
                "plt.ylabel('Memory Utilization (%)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. The Bar Chart (Categorical Distribution)\n",
                "Bar charts are excellent for comparing discrete categories, such as error counts per microservice or cost breakdown per AWS resource."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulating HTTP 500 error counts across different microservices during an outage\n",
                "services = ['auth-api', 'payment-gw', 'search-svc', 'inventory-db']\n",
                "error_counts = [150, 4200, 12, 0]\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "\n",
                "# Using colors to highlight the problem area\n",
                "colors = ['gray', 'red', 'gray', 'gray'] \n",
                "\n",
                "bars = plt.bar(services, error_counts, color=colors)\n",
                "\n",
                "plt.title('HTTP 500 Errors by Service (Outage Incident 409)')\n",
                "plt.xlabel('Microservice')\n",
                "plt.ylabel('Error Count')\n",
                "plt.yscale('log') # Log scale is often used in SRE because differences can be orders of magnitude\n",
                "\n",
                "# Add exact numbers on top of the bars\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, yval + (yval*0.1), int(yval), ha='center', va='bottom')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. The Histogram (Latency Buckets)\n",
                "When looking at latency (response times), averages (means) lie to you. A p99 latency spike can be hidden by thousands of fast requests. Histograms group data into \"buckets\" so you can see the true distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate 1000 API request latencies in milliseconds\n",
                "# Most are fast (around 50ms), but some are very slow (long tail)\n",
                "np.random.seed(42)\n",
                "fast_requests = np.random.normal(loc=50, scale=10, size=900)\n",
                "slow_requests = np.random.normal(loc=300, scale=50, size=100) # The \"long tail\"\n",
                "\n",
                "latencies = np.concatenate([fast_requests, slow_requests])\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "# bins=50 creates 50 slices to group the data into\n",
                "plt.hist(latencies, bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
                "\n",
                "plt.title('API Response Time Distribution (Notice the long tail)')\n",
                "plt.xlabel('Latency (ms)')\n",
                "plt.ylabel('Number of Requests')\n",
                "\n",
                "# Add lines for percentiles\n",
                "p95 = np.percentile(latencies, 95)\n",
                "p99 = np.percentile(latencies, 99)\n",
                "\n",
                "plt.axvline(p95, color='orange', linestyle='dashed', linewidth=2, label=f'p95: {p95:.1f}ms')\n",
                "plt.axvline(p99, color='red', linestyle='dashed', linewidth=2, label=f'p99: {p99:.1f}ms')\n",
                "\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Subplots (Creating a Mini-Dashboard)\n",
                "Often, you need to correlate two metrics side-by-side (e.g., CPU Spikes causing Latency Spikes). We use `plt.subplots()` for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate data\n",
                "time = np.arange(0, 60) # 60 minutes\n",
                "cpu = np.random.normal(40, 5, 60)\n",
                "cpu[30:40] += 50 # massive CPU spike at minute 30\n",
                "\n",
                "latency = np.random.normal(20, 2, 60)\n",
                "latency[30:40] += 80 # Latency follows the CPU spike\n",
                "\n",
                "# Create a 2-row, 1-column layout\n",
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
                "\n",
                "# Top plot: CPU\n",
                "ax1.plot(time, cpu, color='darkblue')\n",
                "ax1.set_title('CPU Utilization %')\n",
                "ax1.set_ylabel('CPU %')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "ax1.axvspan(30, 40, color='red', alpha=0.2, label='Incident Window')\n",
                "ax1.legend()\n",
                "\n",
                "# Bottom plot: Latency\n",
                "ax2.plot(time, latency, color='darkred')\n",
                "ax2.set_title('API Latency (ms)')\n",
                "ax2.set_xlabel('Time (Minutes)')\n",
                "ax2.set_ylabel('Latency (ms)')\n",
                "ax2.grid(True, alpha=0.3)\n",
                "ax2.axvspan(30, 40, color='red', alpha=0.2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}