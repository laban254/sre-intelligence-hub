{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning a Small LLM for Infrastructure Log Parsing\n",
                "\n",
                "## Objectives\n",
                "- Understand the concepts of Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA).\n",
                "- Take a small, pre-trained open-source model (e.g., Llama-3.2-1B or Qwen-2.5-0.5B).\n",
                "- Fine-tune it to automatically extract structured JSON data from messy, unstructured infrastructure logs.\n",
                "\n",
                "## Dataset\n",
                "- A JSONL dataset mapping raw log strings to structured dictionaries containing `severity`, `component`, and `message`.\n",
                "\n",
                "## Expected Outcome\n",
                "- Instead of writing complex Regex rules, we will pass a new log line to our fine-tuned model and it will reliably output perfectly formatted JSON.\n",
                "\n",
                "## Environment Note\n",
                "- *This notebook runs best on a Google Colab T4 GPU (free tier).* If running on a CPU, training will be very slow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install -q transformers datasets peft trl accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. The Training Data\n",
                "In a real scenario, you'd export a few hundred logs from Datadog or Splunk and manually label them to teach the LLM the exact schema you want."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from datasets import Dataset\n",
                "\n",
                "# Sample data showing how we want our LLM to behave\n",
                "training_examples = [\n",
                "    {\n",
                "        \"log\": \"[2025-02-25 10:14:22] ERROR: Connection timeout calling backend service at 10.0.4.55:8080\",\n",
                "        \"json\": '{\"severity\": \"ERROR\", \"component\": \"backend_service\", \"ip\": \"10.0.4.55\", \"error\": \"Connection timeout\"}'\n",
                "    },\n",
                "    {\n",
                "        \"log\": \"[2025-02-25 10:14:28] WARN: High memory usage detected on node worker-3 (88%)\",\n",
                "        \"json\": '{\"severity\": \"WARN\", \"component\": \"node\", \"target\": \"worker-3\", \"metric\": \"memory_usage\", \"value\": \"88%\"}'\n",
                "    },\n",
                "    {\n",
                "        \"log\": \"[2025-02-25 10:14:30] ERROR: OutOfMemoryError: Java heap space thread id 553\",\n",
                "        \"json\": '{\"severity\": \"ERROR\", \"component\": \"jvm\", \"thread_id\": \"553\", \"error\": \"OutOfMemoryError: Java heap space\"}'\n",
                "    }\n",
                "]\n",
                "\n",
                "# Format for causal language modeling prompt\n",
                "formatted_data = []\n",
                "for example in training_examples:\n",
                "    text = f\"Extract structured JSON from this log.\\n\\nLog: {example['log']}\\n\\nJSON: {example['json']}\"\n",
                "    formatted_data.append({\"text\": text})\n",
                "\n",
                "dataset = Dataset.from_list(formatted_data)\n",
                "print(\"Sample training prompt:\\n\")\n",
                "print(dataset[0]['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Loading the Model with LoRA\n",
                "Loading the full model takes too much VRAM. We use `bitsandbytes` to load it in 4-bit precision, and `peft` to only train a tiny fraction of the weights via LoRA adapters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "'''\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" # A very small, highly capable model\n",
                "\n",
                "# 1. Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# 2. Quantization Config (4-bit)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=\"float16\",\n",
                ")\n",
                "\n",
                "# 3. Load Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "# 4. Apply LoRA\n",
                "lora_config = LoraConfig(\n",
                "    r=8, # Rank (how \"wide\" the adapter is)\n",
                "    lora_alpha=16, # Scaling factor\n",
                "    target_modules=[\"q_proj\", \"v_proj\"], # Modules to target\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()\n",
                "'''"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Training Loop (SFTTrainer)\n",
                "Using the Supervised Fine-tuning Trainer from Hugging Face makes the training loop trivial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "'''\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./log-parser-model\",\n",
                "    per_device_train_batch_size=1, # Keep small for 0.5B model on Colab\n",
                "    gradient_accumulation_steps=4,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    learning_rate=2e-4,\n",
                "    max_steps=50, # In reality, you'd train for 1-3 epochs over 500+ samples\n",
                "    logging_steps=10,\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=512,\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "# Train!\n",
                "trainer.train()\n",
                "'''\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Inference (Testing the Fine-Tuned Model)\n",
                "Let's see if the model learned to convert logs into our desired JSON schema without using prompt engineering."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "'''\n",
                "new_log = \"[2025-02-25 10:14:40] ERROR: Database connection failed (FATAL: remaining connection slots are reserved)\"\n",
                "prompt = f\"Extract structured JSON from this log.\\n\\nLog: {new_log}\\n\\nJSON: \"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100)\n",
                "\n",
                "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(response)\n",
                "'''\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}