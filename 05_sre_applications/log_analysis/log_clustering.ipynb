{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Intelligent Log Grouping and Clustering\n",
                "\n",
                "## Objectives\n",
                "- Transform unstructured system logs (text) into numerical features using **TF-IDF**.\n",
                "- Apply **K-Means Clustering** to automatically group similar error messages, ignoring dynamic variables like timestamps, IPs, or process IDs.\n",
                "\n",
                "## Dataset\n",
                "- A curated list of raw server log lines (synthetic but realistic) simulating a fast-moving production environment.\n",
                "\n",
                "## Expected Outcome\n",
                "- Given 10,000 messy log lines, the model will output the top 4 \"root cause\" categories automatically, saving hours of manual `grep` work.\n",
                "\n",
                "## Challenge\n",
                "- Can you apply an \"Elbow Method\" plot to automatically determine the optimal number of clusters (`k`) for your logs?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.cluster import KMeans\n",
                "import re\n",
                "\n",
                "pd.set_option('display.max_colwidth', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating Raw Log Data\n",
                "In production, logs contain dynamic data (like IDs or IPs) that confuse simple text-matching. We need to group log lines that are *structurally* similar."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "raw_logs = [\n",
                "    \"[2025-02-25 10:14:22] ERROR: Connection timeout calling backend service at 10.0.4.55:8080\",\n",
                "    \"[2025-02-25 10:14:23] INFO: User 8943 logged in successfully from 192.168.1.1\",\n",
                "    \"[2025-02-25 10:14:25] ERROR: Connection timeout calling backend service at 10.0.4.56:8080\",\n",
                "    \"[2025-02-25 10:14:28] WARN: High memory usage detected on node worker-3 (88%)\",\n",
                "    \"[2025-02-25 10:14:30] ERROR: OutOfMemoryError: Java heap space thread id 553\",\n",
                "    \"[2025-02-25 10:14:31] INFO: User 1221 logged in successfully from 10.0.0.5\",\n",
                "    \"[2025-02-25 10:14:32] ERROR: Connection timeout calling backend service at 10.0.4.55:8080\",\n",
                "    \"[2025-02-25 10:14:35] ERROR: OutOfMemoryError: Java heap space thread id 992\",\n",
                "    \"[2025-02-25 10:14:38] WARN: High memory usage detected on node worker-1 (92%)\",\n",
                "    \"[2025-02-25 10:14:40] ERROR: Database connection failed (FATAL: remaining connection slots are reserved for non-replication superuser connections)\"\n",
                "] * 10 # Multiply to simulate more logs\n",
                "\n",
                "df = pd.DataFrame({'raw_log': raw_logs})\n",
                "print(f\"Total logs to process: {len(df)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Preprocessing / Log Parsing\n",
                "Before clustering, it's best practice to strip out digits, IPs, and timestamps so the algorithm focuses on the \"skeleton\" of the message."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_log(log_line):\n",
                "    # Remove timestamps [2025...]\n",
                "    log_line = re.sub(r'\\[.*?\\]', '', log_line)\n",
                "    # Remove IP addresses\n",
                "    log_line = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(:\\d+)?\\b', '<IP>', log_line)\n",
                "    # Remove standalone numbers (user IDs, thread IDs, percentages)\n",
                "    log_line = re.sub(r'\\b\\d+\\%?\\b', '<NUM>', log_line)\n",
                "    \n",
                "    return log_line.strip()\n",
                "\n",
                "df['cleaned_log'] = df['raw_log'].apply(clean_log)\n",
                "df[['raw_log', 'cleaned_log']].head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Feature Extraction (TF-IDF)\n",
                "TF-IDF translates our text into a matrix of numbers, weighing rare words (like \"OutOfMemoryError\") higher than common words (like \"at\" or \"from\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer = TfidfVectorizer(max_features=50, stop_words='english')\n",
                "X = vectorizer.fit_transform(df['cleaned_log'])\n",
                "print(f\"TF-IDF Matrix shape: {X.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. K-Means Clustering\n",
                "We group the logs into `k=5` clusters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
                "df['cluster'] = kmeans.fit_predict(X)\n",
                "\n",
                "# Look at the clusters!\n",
                "grouped = df.groupby('cluster')\n",
                "\n",
                "for name, group in grouped:\n",
                "    print(f\"\\n--- Cluster {name} ({len(group)} logs) ---\")\n",
                "    # Show one representative example from this cluster\n",
                "    print(group['cleaned_log'].iloc[0])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}