{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tracking Experiments with MLflow\n",
                "\n",
                "## Objectives\n",
                "- Bridge the gap between \"Data Science\" (building models in a black box) and \"MLOps\" (tracking models like operational software).\n",
                "- Integrate **MLflow** into a Scikit-Learn training script to automatically log hyperparameters, metrics, and models.\n",
                "\n",
                "## Dataset\n",
                "- We will use a synthetic dataset for a binary classification task (e.g., \"Will this server crash in the next 10 minutes?\").\n",
                "\n",
                "## Expected Outcome\n",
                "- A functional Python script that logs training runs. You can view these runs by typing `mlflow ui` in your terminal.\n",
                "\n",
                "## Challenge\n",
                "- Can you modify the logging code to also track the model's signature (the expected input/output schema) using `mlflow.models.signature.infer_signature`?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install mlflow scikit-learn pandas matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
                "from sklearn.datasets import make_classification\n",
                "\n",
                "# Set up MLflow tracking URI (default is a local ./mlruns directory)\n",
                "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\") \n",
                "mlflow.set_experiment(\"Server_Crash_Prediction\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. The Scenario\n",
                "Every time you train a model, you typically try different hyperparameters. Without MLflow, you end up with spreadsheets or messy notebook cells trying to remember what worked best."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Synthetic SRE Data (Features could be latency, memory util, disk IO)\n",
                "X, y = make_classification(n_samples=2000, n_features=10, n_informative=5, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. The MLflow Context Manager\n",
                "By wrapping our training code in `with mlflow.start_run():`, everything inside that block is grouped into a single tracked \"Run\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_log_model(n_estimators, max_depth):\n",
                "    with mlflow.start_run():\n",
                "        # 1. Log the parameters we are testing\n",
                "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
                "        mlflow.log_param(\"max_depth\", max_depth)\n",
                "        \n",
                "        # 2. Train the model\n",
                "        clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
                "        clf.fit(X_train, y_train)\n",
                "        \n",
                "        # 3. Evaluate\n",
                "        predictions = clf.predict(X_test)\n",
                "        acc = accuracy_score(y_test, predictions)\n",
                "        prec = precision_score(y_test, predictions)\n",
                "        rec = recall_score(y_test, predictions)\n",
                "        \n",
                "        # 4. Log the metrics\n",
                "        mlflow.log_metric(\"accuracy\", acc)\n",
                "        mlflow.log_metric(\"precision\", prec)\n",
                "        mlflow.log_metric(\"recall\", rec)\n",
                "        \n",
                "        # 5. Log the actual model artifact!\n",
                "        mlflow.sklearn.log_model(clf, \"random_forest_model\")\n",
                "        \n",
                "        print(f\"Run Complete! | estimators: {n_estimators}, depth: {max_depth} | Acc: {acc:.3f} | Prec: {prec:.3f}\")\n",
                "\n",
                "# Let's simulate a hyperparameter search\n",
                "train_and_log_model(50, 5)\n",
                "train_and_log_model(100, 10)\n",
                "train_and_log_model(200, None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Viewing the Results\n",
                "Now that we have logged 3 runs, you can open your terminal, navigate to this directory, and type:\n",
                "\n",
                "```bash\n",
                "mlflow ui\n",
                "```\n",
                "\n",
                "This will launch a web dashboard on `http://localhost:5000` where you can compare the runs, visualize the metrics, and download the exact `.pkl` model file that produced the best results."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}