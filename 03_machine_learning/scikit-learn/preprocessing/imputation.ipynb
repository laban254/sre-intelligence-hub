{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Imputation for Missing Telemetry\n",
                "\n",
                "## Context\n",
                "In infrastructure monitoring, it is incredibly common to have missing data points. A telemetry agent (like Prometheus Node Exporter) might crash, network partitions might cause dropped packets, or an API might temporarily fail to log its metrics.\n",
                "\n",
                "Machine Learning algorithms generally cannot handle missing values (often represented as `NaN` or `null`). Therefore, before training models or doing advanced analysis, we must **impute** (fill in) these missing values.\n",
                "\n",
                "## Objectives\n",
                "- Simulate a realistic SRE dataset where an agent periodically drops CPU and Memory metrics.\n",
                "- Explore different imputation strategies using Scikit-Learn's `SimpleImputer`:\n",
                "  - Mean Imputation\n",
                "  - Median Imputation (robust to spikes)\n",
                "  - Constant Imputation (filling with 0s or custom values)\n",
                "  - Most Frequent Imputation (for categorical server states)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.model_selection import train_test_split\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generate Synthetic Server Telemetry with Missing Data\n",
                "We will generate data representing Server CPU usage, Memory usage, Disk I/O, and current Status, then artificially drop ~10% of the data to simulate agent failures."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 100\n",
                "\n",
                "# Base telemetry\n",
                "cpu_usage = np.random.normal(loc=45, scale=15, size=n_samples)\n",
                "mem_usage = np.random.normal(loc=60, scale=10, size=n_samples)\n",
                "disk_io = np.random.normal(loc=200, scale=50, size=n_samples)\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'CPU_Usage_pct': cpu_usage,\n",
                "    'Memory_Usage_pct': mem_usage,\n",
                "    'Disk_IO_ops': disk_io\n",
                "})\n",
                "\n",
                "# Introduce missing values randomly (approx 10% chance per cell)\n",
                "mask = np.random.choice([True, False], size=df.shape, p=[0.1, 0.9])\n",
                "df_with_missing = df.mask(mask)\n",
                "\n",
                "print(\"Missing values per column:\")\n",
                "print(df_with_missing.isna().sum())\n",
                "\n",
                "print(\"\\nFirst few rows with missing data (NaN):\")\n",
                "df_with_missing.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Imputation Strategies\n",
                "\n",
                "#### **Mean Imputation**\n",
                "Replaces missing values with the mean of the column. This is standard but can skew data if you have massive anomalies (e.g., sudden 100% CPU spikes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an imputer that uses the 'mean' strategy\n",
                "mean_imputer = SimpleImputer(strategy='mean')\n",
                "\n",
                "# Fit and transform the data\n",
                "df_mean = pd.DataFrame(\n",
                "    mean_imputer.fit_transform(df_with_missing), \n",
                "    columns=df_with_missing.columns\n",
                ")\n",
                "\n",
                "df_mean.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Median Imputation**\n",
                "Replaces missing values with the median. This is highly recommended for infrastructure metrics because server metrics often have extreme outliers (e.g., occasional 10,000ms latency spikes) that would drag the mean upward."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "median_imputer = SimpleImputer(strategy='median')\n",
                "\n",
                "df_median = pd.DataFrame(\n",
                "    median_imputer.fit_transform(df_with_missing), \n",
                "    columns=df_with_missing.columns\n",
                ")\n",
                "\n",
                "df_median.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Constant Imputation (e.g., Zero)**\n",
                "Sometimes, a missing metric explicitly means `0` (e.g., a \"5xx Error Count\" metric that isn't emitted if there are no errors). In this case, we pad missing values with a constant."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "constant_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
                "\n",
                "df_constant = pd.DataFrame(\n",
                "    constant_imputer.fit_transform(df_with_missing), \n",
                "    columns=df_with_missing.columns\n",
                ")\n",
                "\n",
                "df_constant.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Handling Categorical Data (Server States)\n",
                "If we have missing categorical data (e.g., a server's reported health state), we cannot calculate a mean or median. Instead, we use the **most frequent** value (the mode)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cat_data = pd.DataFrame({\n",
                "    'Server_ID': ['srv-1', 'srv-2', 'srv-3', 'srv-4', 'srv-5', 'srv-6'],\n",
                "    'Status': ['Healthy', 'Healthy', np.nan, 'Warning', 'Healthy', np.nan]\n",
                "})\n",
                "\n",
                "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
                "\n",
                "cat_imputed = pd.DataFrame(\n",
                "    cat_imputer.fit_transform(cat_data[['Status']]), \n",
                "    columns=['Status']\n",
                ")\n",
                "\n",
                "cat_data['Status_Imputed'] = cat_imputed\n",
                "cat_data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Important: Imputing Train / Test Splits properly\n",
                "A critical rule in ML is preventing **Data Leakage**. If you fill missing values *before* splitting your dataset, the test set's data will influence the training data's means/medians.\n",
                " \n",
                "**Correct Workflow:**\n",
                "1. Split the data.\n",
                "2. `fit()` the imputer **only on the training set**.\n",
                "3. `transform()` both the training set and the test set using that fitted imputer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create target variable to simulate a supervised learning split\n",
                "y = (df['CPU_Usage_pct'] > 50).astype(int)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(df_with_missing, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# Create Imputer\n",
                "imputer = SimpleImputer(strategy='median')\n",
                "\n",
                "# FIT ONLY ON X_TRAIN, then transform\n",
                "X_train_imputed = imputer.fit_transform(X_train)\n",
                "\n",
                "# TRANSFORM X_TEST using the median calculated from X_TRAIN\n",
                "X_test_imputed = imputer.transform(X_test)\n",
                "\n",
                "# Data is now safely preprocessed without leakage and ready for ML algorithms.\n",
                "print(\"Train Set Shape:\", X_train_imputed.shape)\n",
                "print(\"Test Set Shape:\", X_test_imputed.shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}