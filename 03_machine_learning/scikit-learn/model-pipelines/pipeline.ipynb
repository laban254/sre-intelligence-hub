{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Machine Learning Pipelines in SRE\n",
                "\n",
                "## Context\n",
                "In a production observability pipeline, raw telemetry data is almost never clean. You might have missing Prometheus scrapes, metrics on vastly different scales (percentage vs. bytes), or categorical tags that need encoding.\n",
                "\n",
                "If you build a Machine Learning model to predict Server Health, you must apply the exact same preprocessing steps to real-time production data as you did to your historical training data. If you don't, your model will crash or make garbage predictions. \n",
                "\n",
                "Scikit-Learn **Pipelines** solve this by bundling preprocessing and modeling into a single, executable pipeline that prevents data leakage and ensures consistency.\n",
                "\n",
                "## Objectives\n",
                "- Build a synthetic SRE dataset representing server health and metrics.\n",
                "- Create a single `Pipeline` that automatically:\n",
                "  1. Imputes missing telemetry data.\n",
                "  2. Scales the metrics.\n",
                "  3. Trains a `RandomForestClassifier` to predict server failure.\n",
                "- Perform hyperparameter tuning safely using `GridSearchCV` on the entire pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.compose import ColumnTransformer\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating Infrastructure Data\n",
                "Let's create a dataset where we predict if a server is `Healthy (0)` or `Failing (1)` based on CPU usage and Network Traffic. We will intentionally include missing (`NaN`) values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 200\n",
                "\n",
                "data = pd.DataFrame({\n",
                "    'CPU_Usage': np.random.normal(60, 20, n_samples),\n",
                "    'Network_Bytes': np.random.normal(1_000_000, 500_000, n_samples),\n",
                "    'Failure': np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])\n",
                "})\n",
                "\n",
                "# Introduce missing values to simulate dropped telemetry\n",
                "data.loc[np.random.choice(data.index, 10), 'CPU_Usage'] = np.nan\n",
                "data.loc[np.random.choice(data.index, 15), 'Network_Bytes'] = np.nan\n",
                "\n",
                "X = data[['CPU_Usage', 'Network_Bytes']]\n",
                "y = data['Failure']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "X_train.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Building the Pipeline\n",
                "\n",
                "Instead of manually calling `.fit_transform()` on an imputer, then `.fit_transform()` on a scaler, and finally `.fit()` on a model, we combine them into one object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the steps of the pipeline as a list of tuples: ('name', Transformer/Model object)\n",
                "pipeline = Pipeline([\n",
                "    ('imputer', SimpleImputer(strategy='median')), # Step 1: Fill missing metrics with median\n",
                "    ('scaler', StandardScaler()),                  # Step 2: Standardize scales\n",
                "    ('classifier', RandomForestClassifier(random_state=42)) # Step 3: Train model\n",
                "])\n",
                "\n",
                "# Execute the entire pipeline on the training data\n",
                "pipeline.fit(X_train, y_train)\n",
                "\n",
                "# Evaluate on the test data\n",
                "# The test data is automatically imputed and scaled using the parameters learned from X_train!\n",
                "predictions = pipeline.predict(X_test)\n",
                "\n",
                "print(\"Sample Predictions (0=Healthy, 1=Failing):\", predictions[:10])\n",
                "print(\"Pipeline Test Accuracy: {:.2f}%\".format(pipeline.score(X_test, y_test) * 100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Hyperparameter Tuning with GridSearchCV\n",
                "\n",
                "If we want to find the best imputation strategy ('mean' vs 'median') AND the best Random Forest depth (5 vs 10), we can tune the *entire pipeline* at once.\n",
                "\n",
                "Notice the syntax: `stepname__parametername` (two underscores)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the grid of hyperparameters to search\n",
                "param_grid = {\n",
                "    'imputer__strategy': ['mean', 'median'],          # Tune step 1\n",
                "    'classifier__n_estimators': [50, 100, 200],      # Tune step 3\n",
                "    'classifier__max_depth': [None, 5, 10]           # Tune step 3\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
                "\n",
                "# Fit the grid search to the training data\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(\"\\nBest parameters found:\")\n",
                "print(grid_search.best_params_)\n",
                "\n",
                "print(\"\\nBest Cross-Validation Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
                "\n",
                "# We can now use grid_search.best_estimator_ as our final production pipeline."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary\n",
                "\n",
                "Pipelines are essential for deploying ML in DevOps environments because:\n",
                "- They ensure **Consistency**: Production streaming data gets exactly the same preprocessing as historical Data Lake data.\n",
                "- They prevent **Data Leakage**: Preprocessing state (like scaler means) is strictly isolated during Cross-Validation splits.\n",
                "- They improve **Code readability**: Multiple workflow steps are reduced to a single `.fit()` call."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}