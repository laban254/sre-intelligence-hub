{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ensemble Methods: Bagging (Bootstrap Aggregating)\n",
                "\n",
                "## Context\n",
                "In observability, models can easily overfit to noise. For instance, a single decision tree trained to predict if a server will run out of memory might perfectly memorize past traffic spikes but fail entirely in production.\n",
                "\n",
                "**Bagging (Bootstrap Aggregating)** solves this by:\n",
                "1. Creating multiple subsets of the original data (Bootstrapping).\n",
                "2. Training a weak model (like a Decision Tree) on each subset.\n",
                "3. Aggregating their predictions (Voting for classification, Averaging for regression).\n",
                "\n",
                "The most famous bagging algorithm is the **Random Forest**.\n",
                "\n",
                "## Objectives\n",
                "- Synthesize a dataset for predicting if a server will trigger an OOM (Out of Memory) alert.\n",
                "- Train a standard `DecisionTreeClassifier` and observe its variance/overfitting.\n",
                "- Train a `RandomForestClassifier` (a Bagging ensemble) and compare its stability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating Infrastructure Telemetry\n",
                "We will predict `OOM_Alert` (0=Healthy, 1=Out of Memory) based on `CPU_Usage`, `Memory_Usage`, and `Concurrent_Connections`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 500\n",
                "\n",
                "X = pd.DataFrame({\n",
                "    'CPU_Usage': np.random.normal(60, 20, n_samples),\n",
                "    'Memory_Usage': np.random.normal(70, 15, n_samples),\n",
                "    'Concurrent_Connections': np.random.normal(1500, 500, n_samples)\n",
                "})\n",
                "\n",
                "# If Memory > 85 and Connections > 1800, high chance of OOM\n",
                "y = ((X['Memory_Usage'] > 85) & (X['Concurrent_Connections'] > 1800)).astype(int)\n",
                "\n",
                "# Add some random noise so it isn't a perfect rule\n",
                "flip_indices = np.random.choice(n_samples, size=30, replace=False)\n",
                "y[flip_indices] = 1 - y[flip_indices]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. The Problem with a Single Decision Tree\n",
                "A single decision tree without depth limits will grow until it perfectly memorizes the training data (overfitting). When it sees the test data, performance drops."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_tree = DecisionTreeClassifier(random_state=42)\n",
                "single_tree.fit(X_train, y_train)\n",
                "\n",
                "print(\"Single Tree - Training Accuracy: {:.2f}%\".format(accuracy_score(y_train, single_tree.predict(X_train))*100))\n",
                "print(\"Single Tree - Testing Accuracy: {:.2f}%\".format(accuracy_score(y_test, single_tree.predict(X_test))*100))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Random Forest (Bagged Decision Trees)\n",
                "Random Forest builds many decision trees (e.g., 100). \n",
                "Each tree trains on a random sample of the rows AND a random sample of the columns. The final prediction is a democratic vote among all 100 trees, preventing the model from deeply memorizing noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "print(\"Random Forest - Training Accuracy: {:.2f}%\".format(accuracy_score(y_train, rf_model.predict(X_train))*100))\n",
                "print(\"Random Forest - Testing Accuracy: {:.2f}%\".format(accuracy_score(y_test, rf_model.predict(X_test))*100))\n",
                "\n",
                "# Notice how the Testing Accuracy improves! The Random Forest is much more generalized."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Generic Bagging Classifier\n",
                "You don't have to just bag Decision Trees. You can bag any model (Support Vector Machines, KNN). Scikit-learn provides a wrapper `BaggingClassifier` for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Here we explicitly bag 50 decision trees\n",
                "bagging_model = BaggingClassifier(\n",
                "    estimator=DecisionTreeClassifier(),\n",
                "    n_estimators=50,\n",
                "    random_state=42\n",
                ")\n",
                "bagging_model.fit(X_train, y_train)\n",
                "\n",
                "print(\"Generic Bagging - Testing Accuracy: {:.2f}%\".format(accuracy_score(y_test, bagging_model.predict(X_test))*100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary\n",
                "- **Pros of Bagging:** Massively reduces overfitting, making models highly robust to noise in telemetry data. Works \"out-of-the-box\" very well.\n",
                "- **Cons of Bagging:** Harder to interpret than a single decision tree (you can't easily visualize a forest of 100 trees), and takes more compute power to train."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}