{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ensemble Methods: Boosting\n",
                "\n",
                "## Context\n",
                "While *Bagging* (like Random Forest) trains many independent models in parallel and averages them, **Boosting** trains models **sequentially**. \n",
                "\n",
                "In Boosting, each new model looks at the mistakes made by the previous model and tries specifically to correct them. It turns a sequence of \"weak learners\" (like very shallow decision trees) into a single \"strong learner\".\n",
                "\n",
                "In SRE, Boosting algorithms (specifically Gradient Boosting or XGBoost) are often the reigning champions for tabular telemetry data tasks, like predicting Database Locks or SLA breaches, because they are incredibly accurate.\n",
                "\n",
                "## Objectives\n",
                "- Generate a challenging SRE dataset: Predicting SLA Breaches based on Latency, Retries, and Error Rates.\n",
                "- Train an **AdaBoost (Adaptive Boosting)** model.\n",
                "- Train a **Gradient Boosting** model.\n",
                "- Compare and contrast the two."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating SLA Breach Data\n",
                "Predicting if an API SLA (Service Level Agreement) will breach based on current telemetry."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 1000\n",
                "\n",
                "X = pd.DataFrame({\n",
                "    'P99_Latency_ms': np.random.normal(500, 200, n_samples),\n",
                "    'Error_Rate_pct': np.random.normal(1.5, 1.0, n_samples),\n",
                "    'Retry_Count': np.random.poisson(lam=5, size=n_samples)\n",
                "})\n",
                "\n",
                "# Complex combination leading to an SLA breach (1)\n",
                "y = ((X['P99_Latency_ms'] > 750) | ((X['Error_Rate_pct'] > 2.5) & (X['Retry_Count'] > 8))).astype(int)\n",
                "\n",
                "# Inject some noise to make it harder\n",
                "noise_idx = np.random.choice(n_samples, size=50, replace=False)\n",
                "y[noise_idx] = 1 - y[noise_idx]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. AdaBoost (Adaptive Boosting)\n",
                "AdaBoost uses \"Decision Stumps\" (decision trees with only 1 split, e.g., `if Latency > 750: Breach else Normal`).\n",
                "After the first stump is trained, it heavily weights the specific data points that it got wrong. The next stump focuses specifically on those hard weights. This repeats, and the final model is a weighted sum."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
                "adaboost.fit(X_train, y_train)\n",
                "\n",
                "ada_pred = adaboost.predict(X_test)\n",
                "print(\"AdaBoost Testing Accuracy: {:.2f}%\".format(accuracy_score(y_test, ada_pred)*100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Gradient Boosting\n",
                "Gradient Boosting also builds trees sequentially. However, instead of simply weighting misclassified points, the next tree tries to predict the **residual error** (the mathematical difference between the prediction and the actual value) of the previous tree using Gradient Descent.\n",
                "\n",
                "It is generally more powerful and flexible than AdaBoost."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# learning_rate controls how strongly each successive tree tries to correct the errors of the precursor.\n",
                "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
                "gb_model.fit(X_train, y_train)\n",
                "\n",
                "gb_pred = gb_model.predict(X_test)\n",
                "print(\"Gradient Boosting Testing Accuracy: {:.2f}%\".format(accuracy_score(y_test, gb_pred)*100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary Comparison\n",
                "\n",
                "- **AdaBoost:** \n",
                "  - Focuses on misclassified samples by increasing their weight.\n",
                "  - Simpler, but can be severely derailed by heavy noise/outliers (because it will obsess over trying to fix an unfixable outlier).\n",
                "- **Gradient Boosting:** \n",
                "  - Focuses on minimizing the loss function (residuals) via gradient descent.\n",
                "  - Highly flexible, handles complex SRE relationships gracefully, and produces state-of-the-art results for tabular data.\n",
                "  - Very prone to overfitting if `n_estimators` is too high or `learning_rate` is not tuned properly."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}