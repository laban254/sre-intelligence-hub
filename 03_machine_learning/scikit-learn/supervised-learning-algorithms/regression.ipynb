{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Application Latency with Regression\n",
                "\n",
                "## Context\n",
                "As a Site Reliability Engineer (SRE), you want to understand what drives application latency. You know that CPU utilization, memory usage, the number of concurrent connections, and the database Queries Per Second (QPS) all play a role. By training a regression model on historical data, you can predict what latency will look like under future load scenarios, allowing you to proactively scale resources before latency breaches your SLAs.\n",
                "\n",
                "## Objectives\n",
                "- Generate a synthetic operational dataset mimicking application performance.\n",
                "- Train basic and regularized regression models (Linear, Ridge).\n",
                "- Train a non-linear model (Random Forest Regressor).\n",
                "- Compare model performance using Mean Squared Error (MSE) and R2 Score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.linear_model import LinearRegression, Ridge\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "# Set style\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating Synthetic Operational Data\n",
                "We will simulate 1,000 minutes of telemetry data for our application. Latency will be a function of the other metrics plus some random noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 1000\n",
                "\n",
                "# Features\n",
                "cpu_utilization = np.random.uniform(10, 95, n_samples)  # 10% to 95%\n",
                "memory_usage = np.random.uniform(20, 80, n_samples)     # 20% to 80%\n",
                "concurrent_connections = np.random.poisson(500, n_samples)\n",
                "db_qps = np.random.normal(2000, 500, n_samples)\n",
                "\n",
                "# Formulate Latency (ms): Non-linear relationship with CPU, linear with Connections and DB QPS\n",
                "# When CPU goes above 80%, latency spikes exponentially.\n",
                "latency = (0.5 * concurrent_connections) + (0.01 * db_qps) + (0.2 * memory_usage) + \\\n",
                "          np.where(cpu_utilization > 80, (cpu_utilization - 80)**2 * 2, cpu_utilization) + \\\n",
                "          np.random.normal(0, 15, n_samples)  # Server noise\n",
                "\n",
                "# Create DataFrame\n",
                "df = pd.DataFrame({\n",
                "    'cpu_percent': cpu_utilization,\n",
                "    'memory_percent': memory_usage,\n",
                "    'connections': concurrent_connections,\n",
                "    'db_qps': db_qps,\n",
                "    'latency_ms': latency\n",
                "})\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Visualize the Target Variable vs Features\n",
                "Let's look at how Latency reacts to CPU utilization. You should notice the hockey-stick curve where latency explodes past 80% CPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 5))\n",
                "plt.scatter(df['cpu_percent'], df['latency_ms'], alpha=0.5, color='royalblue')\n",
                "plt.title('Application Latency vs CPU Utilization')\n",
                "plt.xlabel('CPU Utilization (%)')\n",
                "plt.ylabel('Latency (ms)')\n",
                "plt.axvline(x=80, color='r', linestyle='--', label='CPU Threshold')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Splitting the Data\n",
                "We separate our target variable (`latency_ms`) from our features and split them into a training set (80%) and a testing set (20%)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[['cpu_percent', 'memory_percent', 'connections', 'db_qps']]\n",
                "y = df['latency_ms']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Training Models\n",
                "We will evaluate three models to see how they capture the relationships, especially the non-linear CPU degradation.\n",
                "\n",
                "#### **Model A: Linear Regression**\n",
                "The simplest approach. Assumes a straight-line relationship."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lin_reg = LinearRegression()\n",
                "lin_reg.fit(X_train, y_train)\n",
                "y_pred_lin = lin_reg.predict(X_test)\n",
                "\n",
                "print(\"--- Linear Regression ---\")\n",
                "print(f\"MSE: {mean_squared_error(y_test, y_pred_lin):.2f}\")\n",
                "print(f\"R2 Score: {r2_score(y_test, y_pred_lin):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Model B: Ridge Regression**\n",
                "Linear regression with L2 regularization to penalize extremely large coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge_reg = Ridge(alpha=10.0)\n",
                "ridge_reg.fit(X_train, y_train)\n",
                "y_pred_ridge = ridge_reg.predict(X_test)\n",
                "\n",
                "print(\"--- Ridge Regression ---\")\n",
                "print(f\"MSE: {mean_squared_error(y_test, y_pred_ridge):.2f}\")\n",
                "print(f\"R2 Score: {r2_score(y_test, y_pred_ridge):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Model C: Random Forest Regressor**\n",
                "An ensemble of decision trees. It is capable of modeling complex, non-linear relationships (like our CPU threshold spike)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "rf_reg.fit(X_train, y_train)\n",
                "y_pred_rf = rf_reg.predict(X_test)\n",
                "\n",
                "print(\"--- Random Forest Regressor ---\")\n",
                "print(f\"MSE: {mean_squared_error(y_test, y_pred_rf):.2f}\")\n",
                "print(f\"R2 Score: {r2_score(y_test, y_pred_rf):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Model Comparison Analysis\n",
                "Because the underlying data contained a non-linear explosion in latency when CPU went over 80%, the Linear and Ridge regression models struggle to accurately predict high-latency events. \n",
                "\n",
                "Let's visualize the True vs. Predicted values to see this in action."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
                "\n",
                "# Plot Linear Regression\n",
                "axes[0].scatter(y_test, y_pred_lin, alpha=0.5, color='orange')\n",
                "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
                "axes[0].set_title(\"Linear Regression: Actual vs Predicted\")\n",
                "axes[0].set_xlabel(\"Actual Latency (ms)\")\n",
                "axes[0].set_ylabel(\"Predicted Latency (ms)\")\n",
                "\n",
                "# Plot Random Forest\n",
                "axes[1].scatter(y_test, y_pred_rf, alpha=0.5, color='green')\n",
                "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
                "axes[1].set_title(\"Random Forest: Actual vs Predicted\")\n",
                "axes[1].set_xlabel(\"Actual Latency (ms)\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# INSIGHT: The Random Forest points hug the black dotted line (perfect prediction) \n",
                "# much more tightly, especially for higher latency values, indicating it successfully \n",
                "# learned the non-linear CPU degradation rule."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}