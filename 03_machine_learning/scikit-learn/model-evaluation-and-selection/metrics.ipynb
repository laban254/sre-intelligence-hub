{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation: Metrics for SRE\n",
                "\n",
                "## Context\n",
                "In observability and DevOps, the standard Machine Learning metric—**Accuracy**—is usually a terrible trap. \n",
                "\n",
                "Consider an alert system predicting Server Outages. If your servers are healthy 99% of the time, a \"dumb\" model that simply hardcodes `return \"Healthy\"` every single time will achieve **99% Accuracy**. But it will completely fail to detect the 1% of the time your database crashes, making it useless.\n",
                "\n",
                "To build effective ML-driven alerting, we must understand the difference between **Precision**, **Recall**, **F1-Score**, and **Confusion Matrices**.\n",
                "\n",
                "## Objectives\n",
                "- Synthesize a highly imbalanced SRE dataset (Fraud/DDoS detection).\n",
                "- Build a basic classification model.\n",
                "- Understand the implications of False Positives (Alert Fatigue) vs False Negatives (Missed Outages).\n",
                "- Calculate and interpret Precision, Recall, F1, and the ROC-AUC score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating Imbalanced Observability Data\n",
                "Let's simulate 10,000 API requests. Only ~2% of them are malicious (DDoS/Scraping)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "n_samples = 10000\n",
                "\n",
                "# 98% Normal traffic\n",
                "normal_requests = pd.DataFrame({\n",
                "    'Request_Rate': np.random.normal(50, 10, 9800),\n",
                "    'Payload_Size': np.random.normal(1024, 200, 9800),\n",
                "    'Status': 0\n",
                "})\n",
                "\n",
                "# 2% Malicious traffic\n",
                "malicious_requests = pd.DataFrame({\n",
                "    'Request_Rate': np.random.normal(300, 50, 200),\n",
                "    'Payload_Size': np.random.normal(5000, 1000, 200),\n",
                "    'Status': 1\n",
                "})\n",
                "\n",
                "df = pd.concat([normal_requests, malicious_requests])\n",
                "\n",
                "X = df[['Request_Rate', 'Payload_Size']]\n",
                "y = df['Status']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
                "\n",
                "print(\"Percentage of Malicious Requests: {:.2f}%\".format(y.mean() * 100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Training the Detector"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "y_pred = model.predict(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. The Confusion Matrix\n",
                "A confusion matrix tells us exactly the types of mistakes our model made.\n",
                "\n",
                "- **True Negatives (TN):** Model says Normal, Traffic is Normal. (Good)\n",
                "- **True Positives (TP):** Model says Attack, Traffic is Attack. (Good)\n",
                "- **False Positives (FP) [Type I Error]:** Model says Attack, Traffic is Normal. (**Paging engineer at 3 AM for nothing! Alert Fatigue!**)\n",
                "- **False Negatives (FN) [Type II Error]:** Model says Normal, Traffic is Attack. (**System goes down unmonitored!**)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal (0)', 'Attack (1)'], yticklabels=['Normal (0)', 'Attack (1)'])\n",
                "plt.ylabel('Actual Label')\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.title('SRE Alerting Confusion Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Advanced Metrics\n",
                "\n",
                "#### **Precision: Out of all the alerts you fired, how many were real?**\n",
                "Formula: `TP / (TP + FP)`\n",
                "*Why it matters in SRE:* Low precision means high False Positives. This causes **Alert Fatigue**, where engineers start ignoring PagerDuty.\n",
                "\n",
                "#### **Recall (Sensitivity): Out of all the real attacks, how many did you catch?**\n",
                "Formula: `TP / (TP + FN)`\n",
                "*Why it matters in SRE:* Low recall means high False Negatives. This causes **Missed Incidents** and downtime.\n",
                "\n",
                "#### **F1-Score: The harmonic mean of Precision and Recall**\n",
                "Used when you need a balance between the two."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Accuracy:  {:.4f}\".format(accuracy_score(y_test, y_pred)))\n",
                "print(\"Precision: {:.4f} (When I page you, I am correct {}% of the time)\".format(precision_score(y_test, y_pred), round(precision_score(y_test, y_pred)*100)))\n",
                "print(\"Recall:    {:.4f} (Out of all attacks, I caught {}% of them)\".format(recall_score(y_test, y_pred), round(recall_score(y_test, y_pred)*100)))\n",
                "print(\"F1-Score:  {:.4f}\".format(f1_score(y_test, y_pred)))\n",
                "\n",
                "# Notice that while Accuracy is extremely high, Precision/Recall paint the real picture of the alerting system."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. ROC-AUC Score\n",
                "The Receiver Operating Characteristic - Area Under Curve (ROC-AUC) measures the model's ability to distinguish between classes unconditionally across all possible threshold values. \n",
                "- `1.0` is perfect.\n",
                "- `0.5` is random guessing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC-AUC requires predicted probabilities, not just hard labels [0,1]\n",
                "y_prob = model.predict_proba(X_test)[:, 1]\n",
                "\n",
                "roc_auc = roc_auc_score(y_test, y_prob)\n",
                "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary\n",
                "As an SRE/Data Scientist, you must tune your model's threshold depending on the business context:\n",
                "- If missing an attack is catastrophic (e.g., banking), you optimize for **Recall** (even if it means PagerDuty goes off more often).\n",
                "- If alerts are just warnings for a non-critical background job, you optimize for **Precision** to protect your engineers' sleep."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}