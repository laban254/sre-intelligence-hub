{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Dimensionality Reduction for Infrastructure Telemetry\n",
                "\n",
                "## Context\n",
                "Modern infrastructure systems emit dozens or even hundreds of telemetry metrics per server (e.g., CPU, Memory, Disk I/O, Network Latency, Cache misses, Thread counts). This high-dimensional data is difficult to visualize and can slow down machine learning models. \n",
                "\n",
                "As an SRE, we can use Dimensionality Reduction techniques like Principal Component Analysis (PCA) and t-SNE to compress these metrics into 2 or 3 components. This allows us to visually spot anomalies or clusters of misbehaving servers that would be impossible to see in a 50-dimension raw dataset.\n",
                "\n",
                "## Objectives\n",
                "- Generate a dataset with multiple operational metrics across different server roles (Web, DB, Cache).\n",
                "- Use PCA (Principal Component Analysis) to reduce the data to 2 dimensions for visualization.\n",
                "- Understand Explained Variance.\n",
                "- Compare PCA with t-SNE (t-Distributed Stochastic Neighbor Embedding)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Generating High-Dimensional Telemetry\n",
                "We will simulate 6 different metrics across 3 types of servers: Web Servers, Database (DB) Servers, and Cache Servers. Each role has a distinct profile of resource usage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(42)\n",
                "\n",
                "# 200 servers of each role\n",
                "n_servers = 200\n",
                "\n",
                "# Web Servers: High Network, Mod. CPU, Low Disk\n",
                "web_data = pd.DataFrame({\n",
                "    'cpu_usage': np.random.normal(60, 10, n_servers),\n",
                "    'memory_usage': np.random.normal(40, 5, n_servers),\n",
                "    'disk_io': np.random.normal(50, 10, n_servers),\n",
                "    'network_in': np.random.normal(800, 100, n_servers),\n",
                "    'network_out': np.random.normal(900, 120, n_servers),\n",
                "    'active_connections': np.random.normal(1500, 300, n_servers),\n",
                "    'role': 'Web'\n",
                "})\n",
                "\n",
                "# DB Servers: High Disk, High Memory, Low Network In, Mod Network Out\n",
                "db_data = pd.DataFrame({\n",
                "    'cpu_usage': np.random.normal(40, 8, n_servers),\n",
                "    'memory_usage': np.random.normal(85, 5, n_servers),\n",
                "    'disk_io': np.random.normal(800, 150, n_servers),\n",
                "    'network_in': np.random.normal(200, 30, n_servers),\n",
                "    'network_out': np.random.normal(500, 80, n_servers),\n",
                "    'active_connections': np.random.normal(100, 20, n_servers),\n",
                "    'role': 'DB'\n",
                "})\n",
                "\n",
                "# Cache Servers: High Memory, Low Disk, Mod Network\n",
                "cache_data = pd.DataFrame({\n",
                "    'cpu_usage': np.random.normal(25, 5, n_servers),\n",
                "    'memory_usage': np.random.normal(90, 3, n_servers),\n",
                "    'disk_io': np.random.normal(10, 2, n_servers),\n",
                "    'network_in': np.random.normal(400, 50, n_servers),\n",
                "    'network_out': np.random.normal(450, 60, n_servers),\n",
                "    'active_connections': np.random.normal(500, 100, n_servers),\n",
                "    'role': 'Cache'\n",
                "})\n",
                "\n",
                "df = pd.concat([web_data, db_data, cache_data], ignore_index=True)\n",
                "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
                "\n",
                "# Separate features and labels\n",
                "X = df.drop('role', axis=1)\n",
                "y = df['role']\n",
                "\n",
                "# Standardize the features (CRITICAL for PCA because metrics have wildly different scales)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Principal Component Analysis (PCA)\n",
                "PCA takes our 6 metrics and creates new \"Principal Components\". These components are linear combinations of the original metrics, ordered by how much *variance* (information) they capture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X_scaled)\n",
                "\n",
                "print(\"PCA Transformed shape:\", X_pca.shape)\n",
                "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
                "print(f\"Total Variance Explained by 2 components: {sum(pca.explained_variance_ratio_) * 100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### **Visualizing PCA**\n",
                "With just two dimensions, we can plot our servers on a 2D graph. If the dimensionality reduction worked, servers of the same role should cluster together based on their underlying telemetry."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 7))\n",
                "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='Set1', s=60, alpha=0.8)\n",
                "plt.title('PCA Projection of Server Telemetry (6D -> 2D)')\n",
                "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
                "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
                "plt.legend(title='Server Role')\n",
                "plt.show()\n",
                "\n",
                "# SRE INSIGHT: You can clearly see that Web, DB, and Cache servers occupy distinct \n",
                "# regions of the component space. If a \"Web\" server suddenly drifted into the \"DB\" \n",
                "# cluster, you would know immediately it is exhibiting abnormal telemetry!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Understanding Components\n",
                "What do these components actually represent? We can look at the PCA `components_` attribute to see how much each original metric contributes to PC1 and PC2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "component_df = pd.DataFrame(pca.components_, columns=X.columns, index=['PC1', 'PC2'])\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "sns.heatmap(component_df, cmap='coolwarm', annot=True, center=0)\n",
                "plt.title('Feature Contributions to Principal Components')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
                "While PCA is great for maintaining global structure and is computationally fast, t-SNE is a non-linear technique that excels at grouping similar data points tightly together. It is heavily used in visualizing highly complex datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
                "X_tsne = tsne.fit_transform(X_scaled)\n",
                "\n",
                "plt.figure(figsize=(10, 7))\n",
                "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis', s=60, alpha=0.8)\n",
                "plt.title('t-SNE Projection of Server Telemetry')\n",
                "plt.xlabel('t-SNE Dimension 1')\n",
                "plt.ylabel('t-SNE Dimension 2')\n",
                "plt.legend(title='Server Role')\n",
                "plt.show()\n",
                "\n",
                "# Note how t-SNE creates very distinct, tight \"islands\" for each role."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}